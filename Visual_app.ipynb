{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gradio transformers torch Pillow"
      ],
      "metadata": {
        "id": "5iNCauTGZak_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Load the pre-trained model and its processor\n",
        "# A \"processor\" prepares the data (image + text) for the model.\n",
        "# We're using a \"ViLT\" (Vision-and-Language Transformer) model\n",
        "# fine-tuned for visual question answering.\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "\n",
        "# 2. Define the core \"prediction\" function\n",
        "# This function will be called every time a user clicks \"Submit\".\n",
        "def answer_question(image, text):\n",
        "    try:\n",
        "        # 3. Prepare the inputs\n",
        "        # The processor converts the raw image and text query into\n",
        "        # the specific numerical format the model expects.\n",
        "        encoding = processor(image, text, return_tensors=\"pt\")\n",
        "\n",
        "        # 4. Run the model\n",
        "        # We pass the processed inputs to the model...\n",
        "        outputs = model(**encoding)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # 5. Decode the answer\n",
        "        # The model's raw output (\"logits\") is just a set of numbers.\n",
        "        # We find the highest-scoring number (the \"argmax\") and use\n",
        "        # the model's config to turn it back into a readable word.\n",
        "        idx = logits.argmax(-1).item()\n",
        "        answer = model.config.id2label[idx]\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"Sorry, I had trouble processing that. Try a different image or question.\"\n",
        "\n",
        "# 6. Create the Gradio web interface\n",
        "# This one line of code builds the entire UI!\n",
        "iface = gr.Interface(\n",
        "    fn=answer_question,  # The function to call\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\"), # An image upload box (provides a PIL image)\n",
        "        gr.Textbox(label=\"Ask a question about the image...\") # A text input box\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Answer\"), # A text output box\n",
        "    title=\"ðŸ¤– Multimodal AI: Visual Question Answering\",\n",
        "    description=\"Upload an image and ask any question about it. (Model: dandelin/vilt-b32-finetuned-vqa)\"\n",
        ")\n",
        "\n",
        "# 7. Launch the app!\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "vTvUYJDdZey4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}